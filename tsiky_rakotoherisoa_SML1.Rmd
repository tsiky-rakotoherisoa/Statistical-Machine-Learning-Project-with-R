---
title: "Statistical Machine Learning Homework 1"
author: "Tsiky Tafita RAKOTOHERISOA"
date: "2024-12-09"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Exercise 1


# Useful Library
```{r}
library(ggplot2)
library(lattice)
library(caret)
```


# Part 1: Dataset Exploration


**1. Downloading the dataset**
```{r}
dataset <- read.csv("aims-sml-2024-2025-data.csv")
head(dataset)
```

**2. Size of the dataset**
```{r}
n <- dim(dataset)
n
```

***number of rows: 225***

***number of column: 2***

**3. Scatter plot of y versus x**
```{r}
x <- dataset$x
y <- dataset$y
plot(x, y, 
     main = "Scatter Plot of y versus x", 
     xlab = "x-axis Label", 
     ylab = "y-axis Label", 
     pch = 16,          
     col = "blue")    
```
**4. This is a regression task because, firstly, regression consists in the task of modeling the relationship between x and y.**

**Furthermore, $y, x \in \mathbb{R}$. Both of them are numerical variables. So we conclude that this is a regression task.** 

# Part 2: Theoritical Framework

**1. A function space for this task**

$$\mathcal{H}\text{ is a space of the function from that we believe the mapping from the input space to y is coming from.}$$

$\text{In this case, let us suggest a function space as a space of polynomial of some degree p. Mathematically,}$

$$\mathcal{H} = \left\{f \quad s.t. \quad \forall x \in \mathcal{X}, f(x) = a_0 + a_1x^{1} + \cdots + a_px^{p}. \quad \{a_0 \cdots a_p\} \in \mathbb{R},\quad  p \in \mathbb{R} \right\}, \quad \mathcal{X} \text{ the input space} $$

**2. The loss function for this task.**
$$\mathcal{L}: \mathcal{Y} \times \mathcal{Y} \mapsto \mathbb{R}_+$$
$$\quad \quad \quad \quad \quad \quad \quad \quad \quad  (y, f(x)) \rightarrow \mathcal{L}(y, f(x)), \quad \quad f \in \mathcal{H}$$

$$\mathcal{L}(y, f(x)) = \mathcal{L}_2 loss = (y - f(x))^2$$


***Its use:*** 

- $\mathcal{L}_2 loss$  is a standard loss function utilized in most regression tasks since it directs the model to optimize to minimize the squared differences between the predicted and target values.


- Furthermore, it is conductive to the learning process to penalize significantly the presence of outliers.


**3. The theoretical risk**


$$R(f) = \mathbb{E}[\mathcal{L}(y, f(x))]$$
$$R(f) = \int_{\mathcal{X} \times \mathcal{Y}} \mathcal{L}(y,  f(x)) \rho_{XY}(x,y) dxdy$$
$\text{where }  \rho_{x,y}(x,y) \text{ is the probability joint distribution.}$


**4. The expression for the Bayes learning machine in this case.**

$$f^{*}(x) =  \mathbb{E}[Y|X]$$
$$f^{*}(x) = \underset{f \in \mathcal{H}}{\arg \inf} \quad \{R(f)\}$$

**5. The empirical risk**
$$\hat{R}(f) = \frac{1}{n} \sum_{i=1}^{n} \mathcal{L}(y_i, f(x_i))$$
$$\hat{R}(f) = \hat{R}(a) = \frac{1}{n} \sum_{i=1}^{n} (y_i -  \sum_{j=0}^{p}a_j x^{j})^2$$
$$\hat{R}(f) =  \frac{1}{n} (Y - Xa)^T(Y - Xa) $$
$\text{Where}$

***n: number of rows. In our case, n = 225***

***p: degree of polynomial***

$$a = (a_0, a_1, \cdots, a_p)^T  \quad \quad \quad Y = (y_1, y_2,\cdots, y_n)^T$$
$$X = \begin{bmatrix}
1 & x_1^1 & x_1^2 & \cdots & x_1^p \\
1 & x_2^1 & x_2^2 & \cdots & x_2^p \\
1 & x_3^1 & x_3^2 & \cdots & x_3^p \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n^1 & x_n^2 & \cdots & x_n^p
\end{bmatrix}$$

***a: vector containing the parameter***



# Part 3: Estimation and Model Complexity 


**1. The expression for the OLS estimator for this problem**

$$\hat{f} = \hat{f}_{\mathcal{H},n} = \hat{f}_n = \underset{f \in \mathcal{H}}{\arg \min} \quad \{\hat{R}(f)\}$$
$$\hat{f}_n =  \underset{f \in \mathcal{H}}{\arg \min} \quad \{  \frac{1}{n} \sum_{i=1}^{n} \mathcal{L}(y_i, f(x_i))  \}$$
We know that 

$$\hat{R}(f) = \hat{R}(a) = (Y - Xa)^T(Y - Xa)$$
Solving for 

$$\frac{\partial \hat{R}(a)}{\partial a} = 0$$

We obtain 

$$\hat{a} = (X^TX)^{-1}(X^TY)$$



**2. Properties of $\hat{f}(x)$**


***- Since $\hat{f}(x)$ is random variable, we must compute important aspects like its bias and its variance.***

$$\mathbb{B}[\hat{f}(x)] = \mathbb{E}[\hat{f}(x)] - f(x) \text{ and } \mathbb{V}[\hat{f}(x)]$$
***- Also, if we make $\hat{f}$ complex(large p), we make the bias small but the variance is increased.***

***And if we make $\hat{f}$ simple( small p), we make the bias large but the variance is decreased.***


\vspace{1cm}


**3. V-fold cross-validation to determine the optimal complexity for the polynomial regression model.**

$$\text{Using the algorithm of V-fold Cross-validation}$$

```{r}
create_design_matrix <- function(x, p) {
  if (!is.numeric(x)) stop("x must be a numeric vector.")
  n <- length(x)
  matrix_data <- matrix(0, nrow = n, ncol = p + 1)
  for (j in 0:p) {
    matrix_data[, j + 1] <- x^j
  }
  return(matrix_data)
}
```



```{r}
fit_model <- function(X, Y) {
  solve(t(X) %*% X) %*% t(X) %*% Y
}
```




```{r}
set.seed(123)
# Function to compute cross-validation error for a given degree
cv_error <- function(data, p, V = 10) {
  n <- nrow(data)
  folds <- sample(rep(1:V, length.out = n)) # Randomly assign data to folds
  errors <- numeric(V) # Initialize error storage
 
  for (v in 1:V) {
    # Split the data into training and test sets
    train <- data[folds != v, ]
    test <- data[folds == v, ]
   
    # Fit polynomial regression model
    model <- lm(y ~ poly(x, p), data = train)
   
    # Predict on the test set
    predictions <- predict(model, newdata = test)
   
    # Calculate mean squared error for the fold
    errors[v] <- mean((test$y - predictions)^2)
  }
 
  # Return the mean cross-validation error
  return(mean(errors))
}

# Prepare the dataset (assuming 'data' is already loaded with columns x and y)
p_max <- 20              # Maximum degree of polynomial
cv_errors <- sapply(1:p_max, function(p) cv_error(dataset, p)) # Cross-validation errors

# Compute empirical risk for each polynomial degree
empirical_risks <- sapply(1:p_max, function(p) {
  model <- lm(y ~ poly(x, p), data = dataset)
  mean(residuals(model)^2)
})

# Find the optimal polynomial degree
optimal_p <- which.min(cv_errors)

# Output the optimal polynomial degree
cat("Optimal Polynomial Degree:", optimal_p, "\n")

```



**-> Optimal complexity means point where both variance and bias are minimum at the same time, which also called Bias Variance Trade Off**


\vspace{1cm}


**4. Plot of the cross-validation error and the empirical risk as function of p.**


```{r}
# Prepare data for plotting
df <- data.frame(
  Degree = 1:p_max,
  CV_Error = cv_errors,
  Empirical_Risk = empirical_risks
)

# Plot cross-validation error and empirical risk
ggplot(df, aes(x = Degree)) +
  geom_line(aes(y = CV_Error, color = "Cross-Validation Error"), size = 0.7) +
  geom_line(aes(y = Empirical_Risk, color = "Empirical Risk"), size = 0.7) +
  labs(title = "Comparison of Cross-Validation Error and Empirical Risk (p = 10)",
       y = "Error", x = "Polynomial Degree") +
  theme_minimal() +
  scale_color_manual(name = "Metric", values = c("blue", "red"))


```

$$\textbf{Comment: }$$

- This plot shows the distribution between degree and two metrics: Cross-Validation Error (blue line) and Empirical Risk (red dots).

- As the polynomial increases from 1 to around 5, the Cross-Validation error decreases sharply.

- After a polynomial degree of around 5, the Cross-Validation Error stabilizes and remains relatively constant.

- The Empirical Risk is consistently lower than the Cross-Validation Error across all polynomial degrees. 

- Similar to the Cross-Validation Error, the Empirical Risk also decreases and stabilizes as the polynomial degree increases but remains consistently low after a certain degree.



# Part 4: Model Comparison and Evaluation 

**1. Fit and plot the following models on the same plot with the data: Simplest, optimal, complex model**


```{r}
# degree of polynomial
p_simple <- 1
p_opt <- 10
p_complex <- 20

simplest_model <- lm(dataset$y ~ poly(x, p_simple), data = dataset)
optimal_model <- lm(dataset$y ~ poly(x, p_opt), data = dataset)
complex_model <- lm(dataset$y ~ poly(x, p_complex), data = dataset)
```

```{r}
simple_pred <- predict(simplest_model)
opt_pred <- predict(optimal_model)
complex_pred <- predict(complex_model)
```

```{r}
plot(dataset$x, dataset$y, pch = "o", col = "black", main = "Model comparison", xlab = "model", ylab = "prediction")
lines(dataset$x, simple_pred,  col = "blue", lwd = 2, lty = 1)
lines(dataset$x, opt_pred,  col = "red", lwd = 2, lty = 1)
lines(dataset$x, complex_pred,  col = "green", lwd = 2, lty = 1)

legend("topright", legend = c("simplest model", "optimal model", "complex model"), col = c("blue", "red", "green"), lty = c(1, 1, 1), lwd = 1)

```


$$\textbf{Comment on their behaviour: }$$

- For the simplest model(p = 1), represented by the blue line: High bias and underfitting.
 This model severely underfits the data, failing to capture the inherent patterns and fluctuations. It oversimplifies the relationship between the variables, which leads to high bias and potentially high error on both training and test datasets.
 
- For the optimal model(p = 10), shown by the red line.
This model effectively captures the main trends and fluctuations in the data without overfitting. It balances complexity and generalization well, resulting in lower cross-validation errors. This model strikes a good balance between bias and variance, making it suitable for predicting new, unseen data.

- For the complex model(p = 20), shown by the green line.
This model fits the data very closely, including minor fluctuations and noise. While it has a very low empirical risk (training error), it likely overfits the data, capturing noise rather than the true underlying pattern. This can lead to high variance and poor performance on new data.


**2. Perform stochastic hold-out validation with S = 100 splits (70% training, 30% testing). Compute and plot boxplots of the test errors for:**


```{r}
set.seed (19671210)
S <- 100
```


```{r}
# Initialize matrix to store errors
errors_matrix <- matrix(0, nrow = S, ncol = 3)
colnames(errors_matrix) <- c("Simplest", "Optimal", "Complex")

# Perform S replications
for (i in 1:S) {
  # Split data (70% training, 30% testing)
  train_index <- createDataPartition(dataset$y, p = 0.7, list = FALSE)
  train_data <- dataset[train_index, ]
  test_data <- dataset[-train_index, ]
  
  # Simplest model (linear regression, p = 1)
  simplest_model <- lm(y ~ poly(x, 1), data = train_data)
  simplest_predictions <- predict(simplest_model, newdata = test_data)
  simplest_mse <- mean((simplest_predictions - test_data$y)^2)
  
  # Optimal model (degree 10 polynomial regression, p = 10)
  optimal_model <- lm(y ~ poly(x, 10), data = train_data)
  optimal_predictions <- predict(optimal_model, newdata = test_data)
  optimal_mse <- mean((optimal_predictions - test_data$y)^2)
  
  # Overly complex model (degree 20 polynomial regression, p = 20)
  complex_model <- lm(y ~ poly(x, 20), data = train_data)
  complex_predictions <- predict(complex_model, newdata = test_data)
  complex_mse <- mean((complex_predictions - test_data$y)^2)
  
  # Insert MSE into the matrix
  errors_matrix[i, ] <- c(simplest_mse, optimal_mse, complex_mse)
}


boxplot(errors_matrix, 
        main = "Test Errors for Different Polynomial Models",
        xlab = "Model",
        ylab = "Mean Squared Error",
        col = c("red", "green", "lightcoral"),
        names = c("Simplest", "Optimal", "Complex"))

```


# Part 5: Further Analysis


**1. ANOVA on the test errors**
```{r}
require(reshape2)
```


```{r}
aov.method <- aov(value~variable, data=melt(data.frame(errors_matrix)))
anova(aov.method)
summary(aov.method)
TukeyHSD(aov.method, ordered = TRUE)
```

$$\textbf{Comment on the ANOVA test:}$$
- The small p-value (*** < 2.2e-16) suggests that there is a statistically significant difference between the test errors of the three models.

- Tukey's Test reveals:

    - Simplest vs. Optimal: There's a significant difference in test errors, with the simplest model having higher errors.

    - Simplest vs. Complex: There's a significant difference in test errors, with the simplest model again having higher errors.

    - Complex vs. Optimal: There’s a smaller but still significant difference, suggesting the optimal model performs slightly better than the complex model.

**2. The 95% confidence and prediction bands for the data set**

```{r}
# Define the response and best predictor
response <- dataset$y
best_predictor_values <- dataset$x


# Fit the simple linear regression model for the best predictor
best_model <-  lm(response ~ poly(best_predictor_values, 10), data = dataset)

# Generate a sequence of predictor values for smoother bands
x_new <- seq(min(best_predictor_values), max(best_predictor_values), length.out = 100)

# Create a new data frame for predictions
new_data <- data.frame(best_predictor_values = x_new)

# Compute the confidence intervals and prediction intervals
predictions <- predict(
  best_model, 
  newdata = new_data, 
  interval = "confidence", # Confidence bands
  level = 0.95             # 95% confidence level
)

predictions_pred <- predict(
  best_model, 
  newdata = new_data, 
  interval = "prediction", # Prediction bands
  level = 0.95             # 95% prediction level
)


```




```{r}
# Plot the data and the regression line
plot(
  best_predictor_values, response,
  main = paste("Confidence and Prediction Bands for", "best predictor"),
  xlab = "best predictor",
  ylab = "Response (rating)",
  pch = 16, col = "blue"
)

# Add the regression line
lines(x_new, predictions[, "fit"], col = "red", lwd = 2)


# Add the confidence bands
lines(x_new, predictions[, "lwr"], col = "darkgreen", lwd = 2, lty = 2)
lines(x_new, predictions[, "upr"], col = "darkgreen", lwd = 2, lty = 2)


# Add the prediction bands
lines(x_new, predictions_pred[, "lwr"], col = "orange", lwd = 2, lty = 3)
lines(x_new, predictions_pred[, "upr"], col = "orange", lwd = 2, lty = 3)


# Add a legend
legend(
  "topleft",inset=0.02,
  legend = c("Regression Line", "Confidence Bands", "Prediction Bands"),
  col = c("red", "darkgreen", "orange"),
  lty = c(1, 2, 3),
  lwd = 2,
  bty = "n"
)
```



**3. The mathematical expression for**


### The confidence band for a single observation (Xi, Yi) is


$$\hat{y} \pm t_{\alpha/2,n-2}\sqrt{MSE}\sqrt{\frac{1}{n} + \frac{(x - \bar{x})^2}{\sum_{i = 1}^{n} (x_i - \bar{x})^2}}$$

### The prediction band for a single observation (Xi, Yi) is


$$\hat{y} \pm t_{\alpha/2,n-2}\sqrt{MSE}\sqrt{1 + \frac{1}{n} + \frac{(x - \bar{x})^2}{\sum_{i = 1}^{n} (x_i - \bar{x})^2}}$$


**4. Comment extensively on what the confidence and prediction bands reveal about the model.**

**Confidence bands**
- The green dashed lines around the red regression line indicate the 95% confidence interval.

- Narrow confidence bands around the regression line suggest high precision in the model's estimation of the relationship between the predictor and the response.

- As the confidence bands widen, it indicates increased uncertainty in the model's estimates. 

**Prediction bands**
- The orange dotted lines around the red regression line indicate the 95% prediction interval.

- At x = -1.0 to -0.5: Prediction bands are relatively narrow, indicating low variability and high reliability of individual predictions.

- At x = -0.5 to 0.5: Bands widen significantly, suggesting higher variability and increased uncertainty in individual predictions.

- At x = 0.5 to 1.0: Bands narrow again, indicating lower variability and higher reliability of individual predictions.


# Exercice 2

```{r}
#install.packages("kernlab")
library(kernlab)
data(help = "spam")
data <- spam

```
```{r}
y <- data[, ncol(data)]
table(y)
```



**1. The distribution of the response for this data set**




```{r}
barplot(table(y),main = "Bar plot of the response variable", col = c("pink", "blue"), xlab = "Category", ylab = "Count")
```
$\textbf{Comment:}$

- The bar plot shows the distribution of the two categories.

- Category non spam has a high count than category  spam.


**2. Comment on the shape of the data set in terms of the sample size and the dimensionality of the input space.**


```{r}
sample_size <- nrow(data)
num_features <- ncol(data)
sample_size
num_features
```


$\textbf{Comment:}$ 

- The data set contains 4601 observations, which is relatively large. It can allow us for more reliable and generalizable results. 

- It should provide sufficient data for most modeling techniques.

- Also, the data set includes 58 features. This is a moderate number of features.

- It may still allow for capturing complex patterns in the data.


**3. Comment from the statistical perspective on the type of data in the input space**


### First, let us check in the types of the variables.
```{r}
 data_characteristic <- str(data)

```
$\textbf{Comment:}$ 

- The data set includes a mixture of numeric variables (such as frequency counts of specific characters) and a categorical target variable (type) with two levels: "spam" and "non spam".

-  The numeric features likely capture specific characteristics of text messages or emails, which are indicative of spam behavior. 

- The categorical target variable is binary, making this a classic classification problem .


### Furthermore, let us check for the distribution of the data

```{r}
distribution <- function()
{
  for (col_name in colnames(data)) {
  # Check if the column is numeric
  if (is.numeric(data[[col_name]])) {
    # Plot histogram
    hist(data[[col_name]], 
         main = paste("Histogram of", col_name), 
         xlab = col_name, 
         col = 'lightblue', 
         border = 'black', 
         breaks = 20)
  }
}
}

```
$\textbf{Comment:}$ 

- According to the histogram above, we can say that all variables are not normally distributed but left-skewed.  


**4. Using the whole data for training and the whole data for test, building the above four learning machines, then plot the comparative ROC curves on the same grid**


```{r}
# Load required libraries
library(MASS)          
library(e1071)         
library(pROC)
library(caret)
library(klaR)
library(reshape2)
```

```{r}
# Define a function to plot ROC curves
plot_roc_curve <- function(true_labels, predictions, model_name, col) {
  roc_curve <- roc(true_labels, predictions[, 2], levels = rev(levels(true_labels)))
  plot(roc_curve, col = col, main = "Comparative ROC Curves", add = TRUE, print.auc = FALSE)
  return(auc(roc_curve))
}
```

```{r}
# Initialize ROC curve
initial_curve <- function()
{
  plot(0, type = "n", xlab = "1 - Specificity", ylab = "Sensitivity",
  xlim = c(0, 1), ylim = c(0, 1), main = "Comparative ROC Curves")
  abline(a = 0, b = 1, lty = 2, col = "gray")
}

```


```{r}
x <- data[, -ncol(data)]   # All columns except the last
y <- data[, ncol(data)]    # The last column is the target variable


# Ensure `y` is a factor for classification
y <- as.factor(y)

# Train and Test sets (using the whole data)
train_x <- x
train_y <- y
test_x <- x
test_y <- y
```


```{r}
# Check for constant features (to avoid errors like "variable appears constant")
constant_cols <- sapply(x, function(col) length(unique(col)) <= 1)
if (any(constant_cols)) {
  x <- x[, !constant_cols]  # Remove constant columns
}
```


```{r}
# 1. LDA
lda_model <- lda(train_x, grouping = train_y)
lda_pred <- predict(lda_model, test_x)$posterior[, 2]
```


```{r}
# 2. QDA
qda_model <- qda(train_x, grouping = train_y)
qda_pred <- predict(qda_model, test_x)$posterior[, 2]
```


```{r}
# 3. Naive Bayes
nb_model <- naiveBayes(train_x, train_y)
nb_pred <- predict(nb_model, test_x, type = "raw")[, 2]
```


```{r}
# 4. Fisher's Linear Discriminant (FLD) using `caret`
fld_model <- train(train_x, train_y, method = "lda", trControl = trainControl(method = "none"))
fld_pred <- predict(fld_model, test_x, type = "prob")[, 2]
```


```{r}
# Compute ROC curves
roc_lda <- roc(test_y, lda_pred, levels = levels(test_y), direction = "<")
roc_qda <- roc(test_y, qda_pred, levels = levels(test_y), direction = "<")
roc_nb <- roc(test_y, nb_pred, levels = levels(test_y), direction = "<")
roc_fld <- roc(test_y, fld_pred, levels = levels(test_y), direction = "<")

```

```{r}
# Plot ROC curves
plot(roc_lda, col = "red", lwd = 2, main = "Comparative ROC Curves", legacy.axes = TRUE)
plot(roc_qda, col = "blue", lwd = 2, add = TRUE)
plot(roc_nb, col = "green", lwd = 2, add = TRUE)
plot(roc_fld, col = "purple", lwd = 2, add = TRUE)

# Add a legend
legend("bottomright", legend = c("LDA", "QDA", "Naive Bayes", "FLD"), 
       col = c("red", "blue", "green", "purple"), lwd = 2)
```


**5. Comment succinctly on what the ROC curves reveal for this data and argue in light of the theory whether or not that was to be expected.**

- All four models (LDA, QDA, Naive Bayes, and FLD) have ROC curves that are close to the top-left corner, which indicates good classification performance. This reflects high sensitivity (true positive rate) and specificity (low false positive rate).

- From the plot, the curves for QDA (blue) and FLD (purple) appear to have slightly better performance than Naive Bayes (green) and LDA (red), particularly at higher sensitivity levels. This suggests QDA and FLD might better handle the data's structure.


- The curves show that all models perform well, but QDA and FLD might be more appropriate for datasets with non-linear boundaries or feature interactions.


**6. Using set.seed(19671210) along with a 2/3 training 1/3 test in the context stratified stochastic holdout split of the data, compute S = 50 replications of the test error for all the above learning machines.**


```{r}
# Set seed for reproducibility
set.seed(19671210)

# Parameters for splitting the data
epsilon <- 1 / 3                # Proportion of observations in the test set
n <- nrow(spam)
nte <- round(n * epsilon)       # Number of observations in the test set
ntr <- n - nte
S <- 50
x <- spam[, -ncol(spam)]
y <- spam[, ncol(spam)]

# Stochastic hold
test_errors <- matrix(0, nrow = S, ncol = 4)
```



```{r}
for (v in 1:S) {
    # Randomly sample indices for the training set
    id.tr <- sample(1:n, ntr)   # For a sample of ntr indices from {1,2,..,n}
    id.te <- setdiff(1:n, id.tr) # Indices not in the training set
   
    # Create training and test sets
    x_train <- x[id.tr, ]
    y_train <- y[id.tr]
    x_test  <- x[id.te, ]
    y_test  <- y[id.te]
   
   
    # Train LDA model
    LDA <- lda(type ~ ., data = spam[id.tr, ])
    # Make predictions on the test set
    lda_pred <- predict(LDA, x_test)$class
    # Calculate prediction error
    test_errors[v, 1] <- mean(lda_pred != y_test)
    # FLD model using caret
    FLD <- train(x_train, y_train, method = "lda", trControl = trainControl(method = "none"))
    # Predictions for FLD
    fld_pred <- predict(FLD, x_test)
   
    # Train RDA model using klaR package
    rda_model <- rda(x_train, grouping = y_train, gamma = 0, lambda = 0.3)
    
    # Make predictions on the test set
    rda_pred <- predict(rda_model, newdata = x_test)$class
    # Calculate prediction error
    test_errors[v, 2] <- mean(rda_pred != y_test)
    
   
    # Naive Bayes model
    naive_Bayes <- naiveBayes(type ~ ., data = spam[id.tr, ])
    # Predictions for Naive Bayes
    naive_pred <- predict(naive_Bayes, x_test)
    # Calculate prediction error
    test_errors[v, 3] <- mean(naive_pred != y_test)
   

    # Calculate prediction error
    test_errors[v, 4] <- mean(fld_pred != y_test)
}

```


**7- Plot the comparative box plots**

```{r}
  test <- data.frame(test_errors)
  Method<-c('LDA', 'QDA', 'Naive Bayes', "FLD")
  colnames(test) <- Method
```

```{R}
  ggplot(data = melt(test), aes(x=variable, y=value)) + geom_boxplot(aes(fill=variable))+
    labs(x='Model', y=expression(hat(R)[te]))+
    theme(legend.position="none")
```

**8. Comment on the distribution of the test error in light of model complexity**

- The test error distribution for FLD and LDA models is low and consistent, reflecting their simplicity and robustness.

-  Naive Bayes shows higher variability and a higher median test error, indicating it might be less reliable despite its moderate complexity.

- QDA, being more complex, also exhibits moderate performance with a slightly higher error and variability compared to the linear models.

-  This suggests that while complex models like QDA can capture more details, they may also be prone to overfitting, unlike the simpler yet effective FLD and LDA models.


# Exercise 3

**Link of the video: https://youtu.be/PNGGtm6hp6o** 







